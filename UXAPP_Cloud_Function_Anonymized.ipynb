{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#requirements.txt\n",
        "functions-framework==3.*\n",
        "google-api-python-client\n",
        "google-auth-httplib2\n",
        "google-auth-oauthlib\n",
        "google-cloud-secret-manager==2.1.0\n",
        "\n",
        "google-cloud-aiplatform>=1.4.2\n",
        "google-cloud-aiplatform[prediction]>=1.16.0\n",
        "google-generativeai\n",
        "google-cloud-vision\n",
        "google-cloud-language==2.12.0\n",
        "google-cloud-speech\n",
        "SpeechRecognition\n",
        "google-cloud-storage\n",
        "\n",
        "opencv-python==4.8.0.76\n",
        "pandas==1.5.3\n",
        "numpy==1.23.5\n",
        "spacy==3.7.2\n",
        "pt-core-news-lg @ https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.7.0/pt_core_news_lg-3.7.0-py3-none-any.whl\n",
        "en_core_web_lg @ https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl\n",
        "nltk==3.8.1\n",
        "ffmpeg-python==0.2.0\n",
        "moviepy==1.0.3\n"
      ],
      "metadata": {
        "id": "IzYPubYqDkwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdiGzwQSmSqz"
      },
      "outputs": [],
      "source": [
        "import functions_framework\n",
        "from google.cloud import secretmanager\n",
        "\n",
        "import io\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytz\n",
        "\n",
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "import google.generativeai as genai\n",
        "import google.auth\n",
        "import spacy\n",
        "import nltk\n",
        "import cv2\n",
        "import ffmpeg\n",
        "import moviepy.editor as mp\n",
        "\n",
        "from google.auth import default\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud import storage\n",
        "from google.cloud.speech_v2 import SpeechClient\n",
        "from google.cloud.speech_v2.types import cloud_speech\n",
        "from google.api_core.client_options import ClientOptions\n",
        "from google.cloud import language_v2\n",
        "from google.cloud import vision\n",
        "\n",
        "from io import BytesIO\n",
        "from PIL import Image, ImageFont, ImageDraw\n",
        "from datetime import timedelta\n",
        "from datetime import datetime\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "set(pytz.all_timezones_set)\n",
        "\n",
        "def get_google_sheet(SPREADSHEET_ID, RANGE_NAME):\n",
        "\n",
        "\n",
        "  sheet = ''\n",
        "  df = ''\n",
        "  header = ''\n",
        "\n",
        "  try:\n",
        "    creds, _ = default()\n",
        "    client_service_sheet_api = build(\"sheets\", \"v4\", credentials=creds)\n",
        "    service = client_service_sheet_api\n",
        "\n",
        "    # Call the Sheets API\n",
        "    sheet = service.spreadsheets()\n",
        "    result = (\n",
        "        sheet.values()\n",
        "        .get(spreadsheetId=SPREADSHEET_ID, range=RANGE_NAME)\n",
        "        .execute()\n",
        "    )\n",
        "\n",
        "    values = result.get(\"values\", [])\n",
        "    if not values:\n",
        "      print(\"No data found.\")\n",
        "      return\n",
        "\n",
        "    header = result.get('values', [])[0]   # Assumes first line is header!\n",
        "    values = result.get('values', [])[1:]  # Everything else is data.\n",
        "    all_data = []\n",
        "\n",
        "    for col_id, col_name in enumerate(header):\n",
        "\n",
        "        column_data = []\n",
        "        for row in values:\n",
        "            column_data.append(row[col_id])\n",
        "        ds = pd.Series(data=column_data, name=col_name)\n",
        "        all_data.append(ds)\n",
        "    df = pd.concat(all_data, axis=1)\n",
        "\n",
        "  except HttpError as err:\n",
        "    print(err)\n",
        "\n",
        "  return sheet, df, header\n",
        "\n",
        "def download_file(real_file_id):\n",
        "\n",
        "  creds, _ = google.auth.default()\n",
        "\n",
        "  try:\n",
        "    client_service_drive_api = build(\"drive\", \"v3\", credentials=creds)\n",
        "    service = client_service_drive_api\n",
        "    file_id = real_file_id\n",
        "    request = service.files().get_media(fileId=file_id)\n",
        "\n",
        "    file = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(file, request)\n",
        "\n",
        "    done = False\n",
        "    while done is False:\n",
        "      status, done = downloader.next_chunk()\n",
        "      print(f\"Download {int(status.progress() * 100)}.\")\n",
        "\n",
        "  except HttpError as error:\n",
        "    print(f\"An error occurred: {error}\")\n",
        "    file = None\n",
        "\n",
        "  return file.getvalue()\n",
        "\n",
        "def upload_to_bucket(client_storage_api, blob_name, path_to_file, bucket_name):\n",
        "\n",
        "    storage_client = client_storage_api\n",
        "\n",
        "    bucket = storage_client.get_bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_filename(path_to_file)\n",
        "\n",
        "    #returns a public url\n",
        "    return blob.public_url\n",
        "\n",
        "def transcribe_batch_gcs_input_inline_output_v4(\n",
        "    client_speech_api,\n",
        "    project_id: str,\n",
        "    gcs_uri_upload: str,\n",
        "    gcs_uri_download: str,\n",
        "    audio_input,\n",
        "    language\n",
        ") -> cloud_speech.BatchRecognizeResults:\n",
        "\n",
        "    long_audio_uri = f\"{gcs_uri_upload}/audio/{audio_input}\"\n",
        "\n",
        "\n",
        "    lang_code = \"\"\n",
        "    if (language == \"Portuguese\"):\n",
        "        lang_code = \"pt-BR\"\n",
        "    else:\n",
        "        lang_code = \"en-US\"\n",
        "\n",
        "\n",
        "    recognizer_id = f'chirp-{lang_code.lower()}'\n",
        "\n",
        "    # Instantiates a client\n",
        "    client = client_speech_api\n",
        "\n",
        "    config = cloud_speech.RecognitionConfig(\n",
        "        features = cloud_speech.RecognitionFeatures(\n",
        "            enable_automatic_punctuation=True,\n",
        "            enable_word_time_offsets=True\n",
        "        ),\n",
        "        auto_decoding_config=cloud_speech.AutoDetectDecodingConfig(),\n",
        "        language_codes=[lang_code],\n",
        "        model=\"chirp\",\n",
        "      )\n",
        "\n",
        "\n",
        "    file_metadata = cloud_speech.BatchRecognizeFileMetadata(uri=gcs_uri_upload)\n",
        "\n",
        "    request = cloud_speech.BatchRecognizeRequest(\n",
        "        recognizer=f\"projects/{project_id}/locations/us-central1/recognizers/{recognizer_id}\", #\n",
        "        config=config,\n",
        "        files=[file_metadata],\n",
        "        recognition_output_config=cloud_speech.RecognitionOutputConfig(\n",
        "            inline_response_config=cloud_speech.InlineOutputConfig(),\n",
        "        ),\n",
        "    )\n",
        "    response = client.batch_recognize(request=request)\n",
        "\n",
        "    print(\"Waiting for operation to complete... \")\n",
        "    operation = response.result(timeout=1800)\n",
        "\n",
        "    print(\"Transcription completed.\")\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"Current Time =\", current_time)\n",
        "\n",
        "    transcript = []\n",
        "    words_data = {'word': [], 'start_offset': [], 'end_offset': []}\n",
        "    words_offset = pd.DataFrame(words_data)\n",
        "    index_alternative_words = 0\n",
        "    if (len(operation.results[gcs_uri_upload].transcript.results) > 0):\n",
        "        print(\"Transcription loaded.\")\n",
        "    else:\n",
        "        print(\"It was not possible to load transcription.\")\n",
        "\n",
        "    full_transcription = \"\"\n",
        "    audio_input_duration = 0\n",
        "    full_results = operation.results\n",
        "    for result in operation.results[gcs_uri_upload].transcript.results:\n",
        "        if (len(result.alternatives) > 0):\n",
        "\n",
        "            transcript.append(result.alternatives[0].transcript)\n",
        "            alternative = result.alternatives[0]\n",
        "            full_transcription += result.alternatives[0].transcript\n",
        "            audio_input_duration = result.result_end_offset.seconds + result.result_end_offset.microseconds / 1e9\n",
        "\n",
        "            for word_info in alternative.words:\n",
        "\n",
        "                word_with_punktuation = word_info.word\n",
        "                words_with_punktuation = word_tokenize(word_with_punktuation)\n",
        "                words_with_punktuation=[word.lower() for word in words_with_punktuation if word.isalpha()]\n",
        "                for word in words_with_punktuation:\n",
        "                    start_offset = word_info.start_offset\n",
        "                    end_offset = word_info.end_offset\n",
        "                    words_offset.loc[index_alternative_words,'word'] = word\n",
        "                    words_offset.loc[index_alternative_words,'start_offset'] = start_offset.total_seconds()\n",
        "                    words_offset.loc[index_alternative_words,'end_offset'] = end_offset.total_seconds()\n",
        "                index_alternative_words += 1\n",
        "\n",
        "    return transcript, words_offset, full_transcription, audio_input_duration, full_results\n",
        "\n",
        "def generate_srt(full_results, gcs_uri_upload, filename, subtitles_sentence_offset):\n",
        "    subtitles = \"\"\n",
        "    index = 1\n",
        "\n",
        "    for result in full_results[gcs_uri_upload].transcript.results:\n",
        "\n",
        "        if (len(result.alternatives) > 0) and (len(result.alternatives[0].words) > 0):\n",
        "            idx_word = 0\n",
        "            count_word = 0\n",
        "            line = \"\"\n",
        "            start_time_str = ''\n",
        "            end_time_str = ''\n",
        "            start_time = 0\n",
        "            start_seconds = 0\n",
        "            for word in result.alternatives[0].words:\n",
        "                line = line + str(word.word) + \" \"\n",
        "                if (count_word == 0):\n",
        "                    start_time = word.start_offset\n",
        "                    start_seconds = start_time.seconds + start_time.microseconds * 1e-9\n",
        "                    start_time_str = format_time(start_seconds)\n",
        "                    count_word += 1\n",
        "                elif (((idx_word+1) == len(result.alternatives[0].words)) or (count_word == 15)):\n",
        "                    end_time = word.end_offset\n",
        "                    end_seconds = end_time.seconds + end_time.microseconds * 1e-9\n",
        "                    end_time_str = format_time(end_seconds)\n",
        "                    subtitles += f\"{index}\\n{start_time_str} --> {end_time_str}\\n{line}\\n\\n\"\n",
        "                    subtitles_sentence_offset.loc[len(subtitles_sentence_offset)] = [str(line), int(start_seconds), int(end_seconds), '', '']\n",
        "                    index += 1\n",
        "                    count_word = 0\n",
        "                    line = ''\n",
        "                else:\n",
        "                    count_word += 1\n",
        "                idx_word += 1\n",
        "\n",
        "\n",
        "    with open(filename, 'w') as srt_file:\n",
        "        srt_file.write(subtitles)\n",
        "\n",
        "    return subtitles_sentence_offset\n",
        "\n",
        "def format_time(seconds):\n",
        "    time_obj = timedelta(seconds=seconds)\n",
        "    return str(time_obj).replace('.', ',')\n",
        "\n",
        "def get_sus_sentences_from_ai_gemini(\n",
        "    full_transcript,\n",
        "    language,\n",
        "    filename\n",
        ") -> str:\n",
        "\n",
        "    text_to_separate = full_transcript\n",
        "    prompt_en = f\"While you neither invent words nor translate, and stay strict to the original text without removing the words from that text, separate the following text into sentences with no more than 32 tokens:\\n{text_to_separate}\"\n",
        "    prompt_pt = f\"Embora você não invente palavras nem traduza e permaneça rigoroso com o texto original sem remover as palavras desse texto, separe o texto a seguir em frases com no máximo 32 tokens:\\n{text_to_separate}\"\n",
        "    if (language=='Portuguese'):\n",
        "        prompt=prompt_pt\n",
        "    else:\n",
        "        prompt=prompt_en\n",
        "\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "    responses = model.generate_content(\n",
        "            prompt,\n",
        "            generation_config={\n",
        "            \"temperature\": 0.2,\n",
        "            \"max_output_tokens\": 16384,\n",
        "            \"top_p\": 0.2,\n",
        "            \"top_k\": 1,\n",
        "        },\n",
        "        stream=False\n",
        "        )\n",
        "\n",
        "    response_ai_gemini = ''\n",
        "    for response in responses:\n",
        "        response_ai_gemini += response.text + '\\n'\n",
        "\n",
        "    with open(filename, 'w') as srt_file:\n",
        "        srt_file.write(response_ai_gemini)\n",
        "\n",
        "    return response_ai_gemini\n",
        "\n",
        "def analyze_sentiment(client_natural_language_api, text_content: str, language):\n",
        "\n",
        "    client = client_natural_language_api\n",
        "    document_type_in_plain_text = language_v2.Document.Type.PLAIN_TEXT\n",
        "\n",
        "    if (language == \"Portuguese\"):\n",
        "        lang_code = \"pt\"\n",
        "    else:\n",
        "        lang_code = \"en\"\n",
        "\n",
        "    text_content = text_content.strip()\n",
        "    document = {\n",
        "        \"content\": text_content,\n",
        "        \"type_\": document_type_in_plain_text,\n",
        "        \"language_code\": lang_code,\n",
        "    }\n",
        "\n",
        "    encoding_type = language_v2.EncodingType.UTF8\n",
        "\n",
        "    response = client.analyze_sentiment(\n",
        "        request={\"document\": document, \"encoding_type\": encoding_type}\n",
        "    )\n",
        "\n",
        "    sentence_sentiment_score = 0\n",
        "    sentence_sentiment_magnitude = 0\n",
        "\n",
        "    if (response != ''):\n",
        "        sentence_sentiment_score = response.document_sentiment.score\n",
        "        sentence_sentiment_magnitude = response.document_sentiment.magnitude\n",
        "\n",
        "    return sentence_sentiment_score, sentence_sentiment_magnitude\n",
        "\n",
        "def analyze_sentiment_document(client_natural_language_api, text_content: str, language, document_sentences_offset):\n",
        "\n",
        "    client = client_natural_language_api\n",
        "    document_type_in_plain_text = language_v2.Document.Type.PLAIN_TEXT\n",
        "\n",
        "    if (language == \"Portuguese\"):\n",
        "        lang_code = \"pt\"\n",
        "    else:\n",
        "        lang_code = \"en\"\n",
        "\n",
        "    text_content = text_content.strip()\n",
        "    document = {\n",
        "        \"content\": text_content,\n",
        "        \"type_\": document_type_in_plain_text,\n",
        "        \"language_code\": lang_code,\n",
        "    }\n",
        "\n",
        "    encoding_type = language_v2.EncodingType.UTF8\n",
        "\n",
        "    response = client.analyze_sentiment(\n",
        "        request={\"document\": document, \"encoding_type\": encoding_type}\n",
        "    )\n",
        "\n",
        "    document_sentence = ''\n",
        "    document_sentence_sentiment = ''\n",
        "    document_sentence_sentiment_score = 0\n",
        "    document_sentence_sentiment_magnitude = 0\n",
        "\n",
        "    if (response != ''):\n",
        "\n",
        "        for document_sentence_response in response.sentences:\n",
        "            document_sentence = document_sentence_response.text.content\n",
        "            document_sentence_sentiment_score = document_sentence_response.sentiment.score\n",
        "            document_sentence_sentiment_magnitude = document_sentence_response.sentiment.magnitude\n",
        "\n",
        "            document_sentence_sentiment = 'Neutral'\n",
        "            if (document_sentence_sentiment_magnitude >= 0.25):\n",
        "                if (document_sentence_sentiment_score > 0.25):\n",
        "                    document_sentence_sentiment = 'Positive'\n",
        "                if (document_sentence_sentiment_score < -0.25):\n",
        "                    document_sentence_sentiment = 'Negative'\n",
        "            document_sentences_offset.loc[len(document_sentences_offset)] = [document_sentence, -1, -1, document_sentence_sentiment, document_sentence_sentiment_magnitude]\n",
        "\n",
        "\n",
        "    return document_sentences_offset\n",
        "\n",
        "def upload_basic(name, file_type, folder_id, evaluated_video_id):\n",
        "\n",
        "  creds, _ = google.auth.default()\n",
        "\n",
        "  try:\n",
        "    # create drive api client\n",
        "    client_service_drive_api = build(\"drive\", \"v3\", credentials=creds)\n",
        "    service = client_service_drive_api\n",
        "    file_metadata = {\"name\": name, \"parents\": [folder_id]}\n",
        "    media = MediaFileUpload(name, mimetype=file_type)\n",
        "\n",
        "    if (evaluated_video_id == ''):\n",
        "        file = (\n",
        "            service.files()\n",
        "            .create(body=file_metadata, media_body=media, fields=\"id\")\n",
        "            .execute()\n",
        "        )\n",
        "    else:\n",
        "        file = (\n",
        "            service.files()\n",
        "            .update(fileId=evaluated_video_id, media_body=media)\n",
        "            .execute()\n",
        "        )\n",
        "    print(f'File ID: {file.get(\"id\")}')\n",
        "\n",
        "  except HttpError as error:\n",
        "    print(f\"An error occurred: {error}\")\n",
        "    file = None\n",
        "\n",
        "  return file.get(\"id\")\n",
        "\n",
        "def compare_sentence_similarity(sentence1, sentence2, nlp):\n",
        "\n",
        "  # Process the sentences using spaCy\n",
        "  doc1 = nlp(sentence1)\n",
        "  doc2 = nlp(sentence2)\n",
        "\n",
        "  # Create a list of word vectors for each sentence\n",
        "  sentence_vectors = [token.vector for token in doc1 if token.has_vector], [token.vector for token in doc2 if token.has_vector]\n",
        "\n",
        "  # Check if both sentences have at least one vector\n",
        "  if not all(sentence_vectors):\n",
        "    return 0.0\n",
        "\n",
        "  similarity_score = doc1.similarity(doc2)\n",
        "\n",
        "  return similarity_score\n",
        "\n",
        "def gsheet_update_values(spreadsheet_id, range_name, value_input_option, values):\n",
        "\n",
        "  creds, _ = google.auth.default()\n",
        "\n",
        "  try:\n",
        "    client_service_sheet_api = build(\"sheets\", \"v4\", credentials=creds)\n",
        "    service = client_service_sheet_api\n",
        "    body = {\"values\": values}\n",
        "    result = (\n",
        "        service.spreadsheets()\n",
        "        .values()\n",
        "        .update(\n",
        "            spreadsheetId=spreadsheet_id,\n",
        "            range=range_name,\n",
        "            valueInputOption=value_input_option,\n",
        "            body=body,\n",
        "        )\n",
        "        .execute()\n",
        "    )\n",
        "    print(f\"{result.get('updatedCells')} cells updated.\")\n",
        "\n",
        "    return result\n",
        "\n",
        "  except HttpError as error:\n",
        "    print(f\"An error occurred: {error}\")\n",
        "\n",
        "    return error\n",
        "\n",
        "def share_file(real_file_id, real_user, real_domain):\n",
        "\n",
        "  creds, _ = google.auth.default()\n",
        "\n",
        "  try:\n",
        "    client_service_drive_api = build(\"drive\", \"v3\", credentials=creds)\n",
        "    service = client_service_drive_api\n",
        "    ids = []\n",
        "    file_id = real_file_id\n",
        "\n",
        "    def callback(request_id, response, exception):\n",
        "      if exception:\n",
        "        # Handle error\n",
        "        print(exception)\n",
        "      else:\n",
        "        print(f\"Request_Id: {request_id}\")\n",
        "        print(f'Permission Id: {response.get(\"id\")}')\n",
        "        ids.append(response.get(\"id\"))\n",
        "\n",
        "    batch = service.new_batch_http_request(callback=callback)\n",
        "    user_permission = {\n",
        "        \"type\": \"user\",\n",
        "        \"role\": \"reader\",\n",
        "        \"emailAddress\": real_user,\n",
        "    }\n",
        "    batch.add(\n",
        "        service.permissions().create(\n",
        "            fileId=file_id,\n",
        "            body=user_permission,\n",
        "            fields=\"id\",\n",
        "        )\n",
        "    )\n",
        "    domain_permission = {\n",
        "        \"type\": \"domain\",\n",
        "        \"role\": \"reader\",\n",
        "        \"domain\": real_domain,\n",
        "    }\n",
        "    batch.add(\n",
        "        service.permissions().create(\n",
        "            fileId=file_id,\n",
        "            body=domain_permission,\n",
        "            fields=\"id\",\n",
        "        )\n",
        "    )\n",
        "    batch.execute()\n",
        "\n",
        "  except HttpError as error:\n",
        "    print(f\"An error occurred: {error}\")\n",
        "    ids = None\n",
        "\n",
        "  return ids\n",
        "\n",
        "\n",
        "def delete_blob(bucket_name, blob_name):\n",
        "    \"\"\"Deletes a blob from the bucket.\"\"\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    generation_match_precondition = None\n",
        "\n",
        "    blob.reload()\n",
        "    generation_match_precondition = blob.generation\n",
        "\n",
        "    blob.delete(if_generation_match=generation_match_precondition)\n",
        "\n",
        "    print(f\"Blob {blob_name} deleted.\")\n",
        "\n",
        "############\n",
        "@functions_framework.http\n",
        "def uxapp_engine_http(request):\n",
        "    \"\"\"HTTP Cloud Function.\n",
        "    Args:\n",
        "        request (flask.Request): The request object.\n",
        "        <https://flask.palletsprojects.com/en/1.1.x/api/#incoming-request-data>\n",
        "    Returns:\n",
        "        The response text, or any set of values that can be turned into a\n",
        "        Response object using `make_response`\n",
        "        <https://flask.palletsprojects.com/en/1.1.x/api/#flask.make_response>.\n",
        "    \"\"\"\n",
        "    request_json = request.get_json(silent=True)\n",
        "    if request_json and 'product' in request_json:\n",
        "        product = request_json['product']\n",
        "    else:\n",
        "        product = ''\n",
        "    if request_json and 'tasktime' in request_json:\n",
        "        tasktime = request_json['tasktime']\n",
        "    else:\n",
        "        tasktime = ''\n",
        "\n",
        "    if ((product != '') and (tasktime != '')):\n",
        "\n",
        "        print(\"Working on the enviromemt... \")\n",
        "        tz = pytz.timezone('America/Sao_Paulo')\n",
        "        now = datetime.now(tz=tz)\n",
        "        current_time = now.strftime(\"%H:%M:%S\")\n",
        "        job_start_time = current_time\n",
        "        print(\"Current Time =\", job_start_time)\n",
        "\n",
        "        nltk.download('punkt')\n",
        "\n",
        "        SCOPES = [\"https://www.googleapis.com/auth/spreadsheets.readonly\"]\n",
        "\n",
        "        CREDENTIALS_LOCATION = \"\" #change\n",
        "        os.environ[\"CREDENTIALS\"] = CREDENTIALS_LOCATION\n",
        "        SERVICE_ACCOUNT_FILE = os.environ[\"CREDENTIALS\"]\n",
        "        creds = service_account.Credentials.from_service_account_file(\n",
        "              SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
        "        creds, _ = default()\n",
        "\n",
        "        print(\"Starting Google Cloud clients... \")\n",
        "\n",
        "        client_natural_language_api = language_v2.LanguageServiceClient()\n",
        "        client_speech_api = SpeechClient(client_options=ClientOptions(\n",
        "                api_endpoint=f'us-central1-speech.googleapis.com'))\n",
        "        client_storage_api = storage.Client()\n",
        "        client_service_drive_api = build(\"drive\", \"v3\", credentials=creds)\n",
        "        client_service_sheet_api = build(\"sheets\", \"v4\", credentials=creds)\n",
        "        project = \"\" #change\n",
        "        location = \"\" #change\n",
        "        vertexai.init(project=project, location=location)\n",
        "        client_vision_api = vision.ImageAnnotatorClient()\n",
        "\n",
        "        ############\n",
        "        print(\"Retriving data... \")\n",
        "\n",
        "        # The ID and range of a sample spreadsheet.\n",
        "        SPREADSHEET_ID = \"\" #change\n",
        "\n",
        "        RANGE_NAME_SPREADSHEET_PARTICIPANT = \"Participant\"\n",
        "        gsheet, participant_gsheet, participant_header = get_google_sheet(SPREADSHEET_ID, RANGE_NAME_SPREADSHEET_PARTICIPANT)\n",
        "        original_data_participant = participant_gsheet.copy()\n",
        "\n",
        "        RANGE_NAME_SPREADSHEET_EXPERIMENT = \"Experiment\"\n",
        "        gsheet, original_data_experiment_gsheet, SPREADSHEET_HEADER = get_google_sheet(SPREADSHEET_ID, RANGE_NAME_SPREADSHEET_EXPERIMENT)\n",
        "        original_data_experiment = original_data_experiment_gsheet.copy()\n",
        "\n",
        "        ######\n",
        "        print(\"Setting status... \")\n",
        "\n",
        "        original_data_experiment = original_data_experiment.reset_index(drop=True)\n",
        "        index_experiment = 0\n",
        "        count_analysis = 0\n",
        "        RANGE_NAME = ''\n",
        "        for idx1_original_data_experiment, row in original_data_experiment.iterrows():\n",
        "            if ((str(row['Product']) == product) and (str(row['Task Start Time']) == tasktime) and (str(row['Status Script']) == 'In Analysis')):\n",
        "\n",
        "                index_experiment = idx1_original_data_experiment\n",
        "                experiment_id = str(row['Participant']) + ' - ' + str(row['Task Objective'])\n",
        "                video_input = str(row['Participant']) + ' - ' + str(row['Task Objective']) + '.mp4'\n",
        "                original_video_url = str(row['Record Link'])\n",
        "                evaluated_video_url = str(row['Evaluated Link'])\n",
        "                evaluated_audio_url = str(row['Evaluated Audio Link'])\n",
        "                evaluated_subtitle_url = str(row['Evaluated Subtitle Link'])\n",
        "                evaluated_ai_sus_sentences_url = str(row['Evaluated AI SUS Sentences Link'])\n",
        "                participant_id = str(row['Participant'])\n",
        "                RANGE_NAME = RANGE_NAME_SPREADSHEET_EXPERIMENT+'!A' + str(index_experiment+2) + ':' + 'AY' + str(index_experiment+2)\n",
        "                row_original_data_experiment = row\n",
        "                count_analysis = 1\n",
        "        if (count_analysis == 0):\n",
        "            print(\"No experiment to analyze found\")\n",
        "            return 'Nothing to do. Product - Task time: {}!'.format(call_response)\n",
        "\n",
        "        original_data_participant = original_data_participant.reset_index(drop=True)\n",
        "        index_participant = 0\n",
        "        participant_idiom = ''\n",
        "        count_analysis_participant = 0\n",
        "        for idx, row in original_data_participant.iterrows():\n",
        "            if (str(row['E-mail']) == participant_id):\n",
        "                index_participant = idx\n",
        "                participant_idiom = str(row['Native Idiom'])\n",
        "                count_analysis_participant = 1\n",
        "        if (count_analysis_participant == 0):\n",
        "            print(\"No experiment to analyze found\")\n",
        "            return 'Nothing to do. Product - Task time: {}!'.format(call_response)\n",
        "\n",
        "        print(\"Downloading video and font... \" + experiment_id)\n",
        "\n",
        "        original_video_id = original_video_url.split('/')[-2]\n",
        "        stream_video = download_file(original_video_id)\n",
        "        with open(video_input, mode=\"wb\") as f:\n",
        "            f.write(stream_video)\n",
        "\n",
        "        font_input = \"Roboto-Bold.ttf\"\n",
        "        original_font_url = \"\" #change\n",
        "        original_font_id = original_font_url.split('/')[-2]\n",
        "        stream_font = download_file(original_font_id)\n",
        "        with open(font_input, mode=\"wb\") as f:\n",
        "            f.write(stream_font)\n",
        "\n",
        "        print(\"Extracting audio... \" + experiment_id)\n",
        "\n",
        "        audio_input = experiment_id + '.wav'\n",
        "        (\n",
        "          ffmpeg.input(video_input)\n",
        "          .output(audio_input, ac=1,acodec='pcm_s16le',format='wav')\n",
        "          .run(overwrite_output=True)\n",
        "        )\n",
        "\n",
        "        print(\"Working on the audio... \" + experiment_id)\n",
        "        audio_bucket_name = \"\" #change\n",
        "        audio_blob_name = 'audio/'+audio_input\n",
        "        upload_to_bucket(client_storage_api, audio_blob_name, audio_input, audio_bucket_name)\n",
        "\n",
        "        print(\"Setting audio config... \" + experiment_id)\n",
        "\n",
        "        storage_client = storage.Client()\n",
        "        bucket_name = \"\" #change\n",
        "        blobs = storage_client.list_blobs(bucket_name)\n",
        "\n",
        "        filter_dir = \"\"\n",
        "        [blob.name for blob in blobs if filter_dir in blob.name ]\n",
        "\n",
        "        print(\"Transcribing audio... \" + experiment_id)\n",
        "\n",
        "        project_id = \"\" #change\n",
        "        gcs_uri_upload = \"\" + audio_blob_name #change to gs://name/\n",
        "        gcs_uri_download = \"\" #change gs://name/another_name\n",
        "        transcript, words_offset, full_transcript, audio_input_duration, full_results = transcribe_batch_gcs_input_inline_output_v4(client_speech_api, project_id, gcs_uri_upload, gcs_uri_download, audio_input, participant_idiom)\n",
        "\n",
        "        print(\"Creating sentences... \" + experiment_id)\n",
        "\n",
        "        texts = ''\n",
        "        for text in transcript:\n",
        "            texts += text + \" \"\n",
        "\n",
        "        sentences_transcription = sent_tokenize(texts)\n",
        "        sentence_offset = {'sentence': [], 'start_offset': [], 'end_offset': [], 'sentence_sentiment': [], 'sentiment_magnitude': []}\n",
        "\n",
        "        print(\"Creating subtitle file... \" + experiment_id)\n",
        "\n",
        "        # Create file and dataframte\n",
        "        subtitles_sentence_offset = pd.DataFrame(sentence_offset)\n",
        "        subtitle_output = experiment_id + \".srt\"\n",
        "        subtitles_sentence_offset = generate_srt(full_results, gcs_uri_upload, subtitle_output, subtitles_sentence_offset)\n",
        "\n",
        "        # Analysing subtitle sentiments\n",
        "        subtitles_sentence_sentiment = ''\n",
        "        subtitles_sentence_magnitude = ''\n",
        "        subtitles_sentiment_score = 0\n",
        "        for idx_subtitle, row_subtitle in subtitles_sentence_offset.iterrows():\n",
        "            if ((len(row_subtitle) > 0) and (row_subtitle['sentence'] != '')):\n",
        "                subtitle = str(row_subtitle['sentence'])\n",
        "                subtitles_sentiment_score, subtitles_sentiment_magnitude = analyze_sentiment(client_natural_language_api, subtitle, participant_idiom)\n",
        "                subtitles_sentence_sentiment = 'Neutral'\n",
        "                if (subtitles_sentiment_magnitude >= 0.25):\n",
        "                    if (subtitles_sentiment_score > 0.25):\n",
        "                        subtitles_sentence_sentiment = 'Positive'\n",
        "                    if (subtitles_sentiment_score < -0.25):\n",
        "                        subtitles_sentence_sentiment = 'Negative'\n",
        "                subtitles_sentence_offset.loc[idx_subtitle, 'sentence_sentiment'] = subtitles_sentence_sentiment\n",
        "                subtitles_sentence_offset.loc[idx_subtitle, 'sentiment_magnitude'] = subtitles_sentiment_magnitude\n",
        "\n",
        "        print(\"AI SUS Similarity - Bard API... \" + experiment_id)\n",
        "\n",
        "        # SUS similarity\n",
        "        ai_sentences_offset = pd.DataFrame(sentence_offset)\n",
        "        ai_sus_sentences = ''\n",
        "        ai_gemini_sus_sentences = ''\n",
        "        ai_gemini_sus_sentences_filename = experiment_id + \" - AI Gemini Subtitle.txt\"\n",
        "        ai_gemini_sus_sentences = get_sus_sentences_from_ai_gemini(full_transcript, participant_idiom, ai_gemini_sus_sentences_filename)\n",
        "        ai_sus_sentences_filename = ai_gemini_sus_sentences_filename\n",
        "        ai_sus_sentences = ai_gemini_sus_sentences\n",
        "\n",
        "        print(\"AI SUS Similarity - Natural Language API... \" + experiment_id)\n",
        "\n",
        "        ai_sentences_offset = analyze_sentiment_document(client_natural_language_api, ai_sus_sentences, participant_idiom, ai_sentences_offset)\n",
        "\n",
        "        print(\"Analyzing SUS similarity... \" + experiment_id)\n",
        "\n",
        "        if (participant_idiom == \"Portuguese\"):\n",
        "            nlp = spacy.load(\"pt_core_news_lg\")\n",
        "            sentence_set1 = [\"Eu acho que gostaria de usar esse sistema com frequência.\", \"Eu acho o sistema desnecessariamente complexo.\", \"Eu achei o sistema fácil de usar.\", \"Eu acho que precisaria de ajuda de uma pessoa com conhecimentos técnicos para usar o sistema.\", \"Eu acho que as várias funções do sistema estão muito bem integradas.\", \"Eu acho que o sistema apresenta muita inconsistência.\", \"Eu imagino que as pessoas aprenderão como usar esse sistema rapidamente.\", \"Eu achei o sistema atrapalhado de usar.\", \"Eu me senti confiante ao usar o sistema.\", \"Eu precisei aprender várias coisas novas antes de conseguir usar o sistema.\"]\n",
        "        else:\n",
        "            nlp = spacy.load(\"en_core_web_lg\")\n",
        "            sentence_set1 = [\"I think that I would like to use this system frequently.\", \"I found the system unnecessarily complex.\", \"I thought the system was easy to use.\", \"I think that I would need the support of a technical person to be able to use this system.\", \"I found the various functions in this system were well-integrated.\", \"I thought there was too much inconsistency in this system.\", \"I would imagine that most people would learn to use this system very quickly.\", \"I found the system very cumbersome to use.\", \"I felt very confident using the system.\", \"I needed to learn a lot of things before I could get going with this system.\"]\n",
        "\n",
        "        # Compare each sentence in the first set to each sentence in the second set\n",
        "        question = 1\n",
        "        sus_identified_data = {'question': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'score': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], 'similarity': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'sentence1': sentence_set1, 'sentence2': ['', '', '', '', '', '', '', '', '', ''], 'sentence1_sentiment': ['', '', '', '', '', '', '', '', '', ''], 'sentence2_sentiment': ['', '', '', '', '', '', '', '', '', '']}\n",
        "        sus_identified = pd.DataFrame(data=sus_identified_data)\n",
        "        for sent1 in sentence_set1:\n",
        "          for idx_sentences_offset, row_sentences_offset in ai_sentences_offset.iterrows():\n",
        "            sent2 = row_sentences_offset['sentence']\n",
        "            sentiment2_magnitude = row_sentences_offset['sentiment_magnitude']\n",
        "            sentence2_sentiment = row_sentences_offset['sentence_sentiment']\n",
        "            similarity_score = 0\n",
        "            similarity_score = compare_sentence_similarity(sent1, sent2, nlp)\n",
        "            if (similarity_score >= 0.25):\n",
        "                if (sentiment2_magnitude >= 0.25):\n",
        "                    score = 3\n",
        "                    sentence1_sentiment = 'Neutral'\n",
        "                    if (sentence2_sentiment == 'Positive'):\n",
        "                        if(question % 2 == 0):\n",
        "                            sentence1_sentiment = 'Negative'\n",
        "                            if (similarity_score >= 0.750):\n",
        "                                score = 1\n",
        "                            else:\n",
        "                                score = 2\n",
        "                        else:\n",
        "                            sentence1_sentiment = 'Positive'\n",
        "                            if (similarity_score >= 0.750):\n",
        "                                score = 5\n",
        "                            else:\n",
        "                                score = 4\n",
        "                    if (sentence2_sentiment == 'Negative'):\n",
        "                        if(question % 2 == 0):\n",
        "                            sentence1_sentiment = 'Negative'\n",
        "                            if (similarity_score >= 0.750):\n",
        "                                score = 5\n",
        "                            else:\n",
        "                                score = 4\n",
        "                        else:\n",
        "                            sentence1_sentiment = 'Positive'\n",
        "                            if (similarity_score >= 0.750):\n",
        "                                score = 1\n",
        "                            else:\n",
        "                                score = 2\n",
        "\n",
        "                    if (sus_identified.loc[question-1]['similarity'] < similarity_score):\n",
        "                        sus_identified.loc[sus_identified.question==question, 'score'] = score\n",
        "                        sus_identified.loc[sus_identified.question==question, 'similarity'] = similarity_score\n",
        "                        sus_identified.loc[sus_identified.question==question, 'sentence1'] = sent1\n",
        "                        sus_identified.loc[sus_identified.question==question, 'sentence2'] = sent2\n",
        "                        sus_identified.loc[sus_identified.question==question, 'sentence1_sentiment'] = sentence1_sentiment\n",
        "                        sus_identified.loc[sus_identified.question==question, 'sentence2_sentiment'] = sentence2_sentiment\n",
        "          question += 1\n",
        "\n",
        "        print(\"Working on video... \" + experiment_id)\n",
        "\n",
        "        frame_number = 0\n",
        "        evaluated_frames = 0\n",
        "\n",
        "        sentiment = {\n",
        "            'valence': [],\n",
        "            'time': [],\n",
        "            'instant_satisfaction': [],\n",
        "            'instant_satisfaction_score': [],\n",
        "            'instant_offset': 0\n",
        "        }\n",
        "        video_sentiment = pd.DataFrame(sentiment)\n",
        "\n",
        "        message_line1 = ''\n",
        "        message_line2 = ''\n",
        "        message_line3 = ''\n",
        "        message_line4 = ''\n",
        "        message_line5 = ''\n",
        "        message_line6 = ''\n",
        "        client = client_vision_api\n",
        "\n",
        "        videoName = video_input\n",
        "\n",
        "        capture = cv2.VideoCapture(video_input)\n",
        "        success, frame = capture.read()\n",
        "        frames = 1\n",
        "\n",
        "        video_width_frame = 1920\n",
        "        ratio_frame = frame.shape[1] / frame.shape[0]\n",
        "        video_height_frame = int(video_width_frame / ratio_frame)\n",
        "\n",
        "        video_width = frame.shape[1]\n",
        "        video_height = frame.shape[0]\n",
        "\n",
        "        video_output = videoName + ' - Output.avi'\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "        fps = capture.get(cv2.CAP_PROP_FPS)\n",
        "        output_video = cv2.VideoWriter(video_output, fourcc, fps, (video_width, video_height))\n",
        "\n",
        "        subtitles_sentence_offset_frame = subtitles_sentence_offset\n",
        "        subtitles_sentence_offset_frame['fps'] = fps\n",
        "        subtitles_sentence_offset_frame = subtitles_sentence_offset.eval('start_frame_offset = start_offset * fps ')\n",
        "        subtitles_sentence_offset_frame = subtitles_sentence_offset_frame.eval('end_frame_offset = end_offset * fps ')\n",
        "        subtitles_sentence_offset_frame.eval('start_frame_offset = start_frame_offset.round(0)', engine='python', inplace=True)\n",
        "        subtitles_sentence_offset_frame.eval('end_frame_offset = end_frame_offset.round(0)', engine='python', inplace=True)\n",
        "        subtitles_sentence_offset_frame = subtitles_sentence_offset_frame.astype({'start_frame_offset':'int','end_frame_offset':'int'})\n",
        "\n",
        "        while success:\n",
        "            image_to_draw = frame\n",
        "            cv2_rgb = cv2.cvtColor(image_to_draw,cv2.COLOR_BGR2RGB)\n",
        "            pil = Image.fromarray(cv2_rgb)\n",
        "            draw = ImageDraw.Draw(pil)\n",
        "\n",
        "            if (frame_number == 0):\n",
        "\n",
        "                crop = pil\n",
        "                crop = crop.crop((1435, 400, 1920, 675))\n",
        "\n",
        "                base_width= 1920\n",
        "                wpercent = (base_width / float(crop.size[0]))\n",
        "                hsize = int((float(crop.size[1]) * float(wpercent)))\n",
        "                crop_resized = crop.resize((base_width, hsize), Image.Resampling.LANCZOS)\n",
        "\n",
        "                b = io.BytesIO()\n",
        "                crop_resized.save(b, 'png')\n",
        "                im_bytes = b.getvalue()\n",
        "                content = im_bytes\n",
        "                image = vision.Image(content=content)\n",
        "\n",
        "                response = client.face_detection(image=image)\n",
        "                faces = response.face_annotations\n",
        "                evaluated_frames += 1\n",
        "\n",
        "                if (len(faces) > 0):\n",
        "\n",
        "                    face = faces[0]\n",
        "                    emotion_intensity = vision.Likelihood.LIKELY\n",
        "                    instant_affect = 'Neutral'\n",
        "                    emotion_valence = 0\n",
        "                    emotion_confidence = face.detection_confidence\n",
        "                    valence = 'neutral'\n",
        "                    if face.joy_likelihood > emotion_intensity:\n",
        "                        emotion_intensity = face.joy_likelihood\n",
        "                        instant_affect = 'Joy'\n",
        "                        emotion_valence = 1\n",
        "                        valence = 'positive'\n",
        "                    if face.sorrow_likelihood > emotion_intensity:\n",
        "                        emotion_intensity = face.sorrow_likelihood\n",
        "                        instant_affect = 'Sorrow'\n",
        "                        emotion_valence = -1\n",
        "                        valence = 'negative'\n",
        "                    if face.anger_likelihood > emotion_intensity:\n",
        "                        emotion_intensity = face.anger_likelihood\n",
        "                        instant_affect = 'Anger'\n",
        "                        emotion_valence = -1\n",
        "                        valence = 'negative'\n",
        "                    if face.surprise_likelihood > emotion_intensity:\n",
        "                        emotion_intensity = face.surprise_likelihood\n",
        "                        instant_affect = 'Surprise'\n",
        "                        emotion_valence = 0\n",
        "                        valence = 'neutral'\n",
        "                    if (face.joy_likelihood == face.sorrow_likelihood) and (face.sorrow_likelihood == face.anger_likelihood) and (face.anger_likelihood == face.surprise_likelihood):\n",
        "                        instant_affect = 'Neutral'\n",
        "                        emotion_valence = 0\n",
        "                        valence = 'neutral'\n",
        "\n",
        "                    # Transform emotion intensity in a number\n",
        "                    intensity_score = 0\n",
        "                    if ((emotion_intensity == vision.Likelihood.VERY_LIKELY)):\n",
        "                            intensity_score = 1\n",
        "\n",
        "                    # Identify concidence score\n",
        "                    minutes = int(frames / (fps * 60))\n",
        "                    seconds = int(int(frames % (fps * 60)) / fps)\n",
        "                    total_seconds = int(frames / fps)\n",
        "                    time = str(minutes) + \"m\" + str(seconds) + \"s\"\n",
        "                    confidence_score = 0\n",
        "                    if (emotion_confidence >= 0.5):\n",
        "                        confidence_score = 1\n",
        "\n",
        "                    instant_satisfaction_score = 0\n",
        "                    instant_satisfaction_score = intensity_score * emotion_valence * confidence_score\n",
        "\n",
        "                    instant_satisfaction = 'Neither satisfied nor dissatisfied'\n",
        "                    if (instant_satisfaction_score == -1):\n",
        "                            instant_satisfaction = 'Dissatisfied'\n",
        "                    elif (instant_satisfaction_score == 1):\n",
        "                            instant_satisfaction = 'Satisfied'\n",
        "\n",
        "                    instant_message = ''\n",
        "\n",
        "                    if ((valence == 'positive') or (valence == 'negative')):\n",
        "                        video_sentiment.loc[len(video_sentiment.index)] = [valence, time, instant_satisfaction, instant_satisfaction_score, total_seconds]\n",
        "\n",
        "                    if response.error.message:\n",
        "                        raise Exception(\n",
        "                            \"{}\\nFor more info on error messages, check: \"\n",
        "                            \"https://cloud.google.com/apis/design/errors\".format(response.error.message)\n",
        "                        )\n",
        "\n",
        "                    if (confidence_score == 0):\n",
        "                        message_line1 = 'Wait for a better visualization.'\n",
        "                        message_line2 = 'Low Confidence Score'\n",
        "                        message_line3 = ''\n",
        "                    else:\n",
        "\n",
        "                        if len(instant_satisfaction) > 0:\n",
        "                            message_line1 = \"Satisfaction: \" + instant_satisfaction\n",
        "                        if len(instant_satisfaction) > 0:\n",
        "                            message_line2 = \"Affect (face): \" +  instant_affect\n",
        "                else:\n",
        "                    message_line1 = 'Wait for a face detection.'\n",
        "                    message_line2 = ''\n",
        "                    message_line3 = ''\n",
        "\n",
        "            message_line3 = ''\n",
        "            message_line4 = ''\n",
        "            message_line5 = ''\n",
        "            mask1 = subtitles_sentence_offset_frame['start_frame_offset'].values <= frames\n",
        "            row_subtitle_sentences_offset_frame = subtitles_sentence_offset_frame[mask1]\n",
        "            row_subtitle_sentences_offset_frame.dropna()\n",
        "            row_subtitle_sentences_offset_frame.reset_index(drop=True)\n",
        "            if (len(row_subtitle_sentences_offset_frame) > 0):\n",
        "                mask2 = row_subtitle_sentences_offset_frame['end_frame_offset'].values > frames\n",
        "                row_subtitle_sentences_offset_frame = row_subtitle_sentences_offset_frame[mask2]\n",
        "                row_subtitle_sentences_offset_frame.dropna()\n",
        "            affect_text_message = ''\n",
        "            row_subtitle_sentences_offset_frame.reset_index(drop=True)\n",
        "            sentence_message = ''\n",
        "            if (len(row_subtitle_sentences_offset_frame) > 0):\n",
        "                affect_text_message = str(row_subtitle_sentences_offset_frame.iloc[0]['sentence_sentiment'])\n",
        "                message_line3 = \"Affect (speech): \" + affect_text_message\n",
        "                # Subtitles\n",
        "                sentence_message = str(row_subtitle_sentences_offset_frame.iloc[0]['sentence'])\n",
        "                words_message = word_tokenize(sentence_message)\n",
        "                words=[word.lower() for word in words_message if word.isalpha()]\n",
        "                message_line4 = ''\n",
        "                message_line5 = ''\n",
        "                if (len(words_message) >= 8):\n",
        "                    size_message4 = math.ceil(len(words_message)/2)\n",
        "                    if not (words_message[size_message4].isalpha()):\n",
        "                        size_message4 += 1\n",
        "                    size_message5 = len(words_message) - size_message4\n",
        "                    size_message = len(words_message)\n",
        "                    word_count = 1\n",
        "                    for word_message in words_message:\n",
        "                        if (word_count < size_message4):\n",
        "                            if (words_message[word_count].isalpha()):\n",
        "                                message_line4 += word_message + \" \"\n",
        "                            else:\n",
        "                                message_line4 += word_message\n",
        "                        elif (word_count == size_message4):\n",
        "                            message_line4 += word_message\n",
        "\n",
        "                        else:\n",
        "                            if (word_count < size_message):\n",
        "                                if (words_message[word_count].isalpha()):\n",
        "                                    message_line5 += word_message + \" \"\n",
        "                                else:\n",
        "                                    message_line5 += word_message\n",
        "                            elif (word_count == size_message):\n",
        "                                message_line5 += word_message\n",
        "                        word_count += 1\n",
        "                else:\n",
        "                    message_line4 = sentence_message\n",
        "                    message_line5 = ''\n",
        "\n",
        "            if ((message_line1 != '') or (message_line2 != '') or (message_line3 != '') or (message_line4 != '') or (message_line5 != '')):\n",
        "                alpha = 0.60\n",
        "                x,y,w,h = round(video_width-660),15,650,70\n",
        "                font = ImageFont.truetype(font_input, 28)\n",
        "\n",
        "                shape1 = [(x, y), (x + w, y + h + 80)]\n",
        "                draw.rectangle(shape1, fill =\"#000000\")\n",
        "                draw.text((x + int(w/10),y + int(h/1.75)), message_line1, font=font)\n",
        "                draw.text((x + int(w/10),y + int(h/1.75) + int(h/2)), message_line2, font=font)\n",
        "                draw.text((x + int(w/10),y + int(h/1.75) + int(h/2) + int(h/2)), message_line3, font=font)\n",
        "\n",
        "                if ((message_line4 != '') or (message_line5 != '')):\n",
        "                    x1,y1,w1,h1 = round(video_width/4),30,800,15\n",
        "\n",
        "                    shape2 = [(x1, video_height - 150), (x1 + w1, video_height - 50)]\n",
        "                    draw.rectangle(shape2, fill =\"#000000\")\n",
        "                    draw.text((x1 + int(w1/10), video_height - 130), message_line4, font=font)\n",
        "                    draw.text((x1 + int(w1/10), video_height - 100), message_line5, font=font)\n",
        "\n",
        "                overlay = cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)\n",
        "                cv2.addWeighted(overlay, alpha, frame, 1 - alpha,0, frame)\n",
        "\n",
        "            if (frame_number == round(1.5*fps) -1):\n",
        "                frame_number = 0\n",
        "            else:\n",
        "                frame_number += 1\n",
        "\n",
        "            output_video.write(frame)\n",
        "\n",
        "            success, frame = capture.read()\n",
        "            frames += 1\n",
        "\n",
        "            if not success:\n",
        "                break\n",
        "\n",
        "        user_value_identified_score = 0\n",
        "        user_value_identified = 'Neither satisfied nor dissatisfied'\n",
        "        if (len(video_sentiment) > 0):\n",
        "            user_value_identified_score = round(video_sentiment['instant_satisfaction_score'].sum() / len(video_sentiment.index),0)\n",
        "\n",
        "            if (user_value_identified_score == -1):\n",
        "                    user_value_identified = 'Dissatisfied'\n",
        "            elif (user_value_identified_score == 1):\n",
        "                    user_value_identified = 'Satisfied'\n",
        "\n",
        "        print(\"Video Input: \" + video_input)\n",
        "        print(\"Video Output: \" + video_output)\n",
        "        print(\"Evaluated frames: \" + str(evaluated_frames))\n",
        "        print(\"Overall satisfaction: \" + user_value_identified)\n",
        "        print(\"Sentiment Peaks: \")\n",
        "        print(video_sentiment)\n",
        "        print(\"Geral Satisfaction: \" + user_value_identified)\n",
        "        print(\"Finish\")\n",
        "        output_video.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "        print(\"Find positive and negative user value satisfaction... \" + experiment_id)\n",
        "\n",
        "        video_sentiment_positive = video_sentiment.mask(video_sentiment['valence'] != \"positive\")\n",
        "        video_sentiment_positive = video_sentiment_positive.dropna()\n",
        "\n",
        "        video_sentiment_negative = video_sentiment.mask(video_sentiment['valence'] != \"negative\")\n",
        "        video_sentiment_negative = video_sentiment_negative.dropna()\n",
        "\n",
        "        sentiment_peak_positive_score = 0\n",
        "        sentiment_peak_negative_score = 0\n",
        "        sentiment_duration = {\n",
        "            'valence': [],\n",
        "            'time': [],\n",
        "            'instant_satisfaction': [],\n",
        "            'instant_satisfaction_score': [],\n",
        "            'duration': []\n",
        "        }\n",
        "\n",
        "        # All positive and negative intervals\n",
        "        all_sentiment_positive_duration_intervals = pd.DataFrame(sentiment_duration)\n",
        "        all_sentiment_positive_duration = 0\n",
        "        all_sentiment_peak_positive_score = 0\n",
        "\n",
        "        all_sentiment_negative_duration_intervals = pd.DataFrame(sentiment_duration)\n",
        "        all_sentiment_negative_duration = 0\n",
        "        all_sentiment_peak_negative_score = 0\n",
        "\n",
        "        video_sentiment_positive.reset_index(drop=True, inplace=True)\n",
        "        for ind in video_sentiment_positive.index:\n",
        "            if (video_sentiment_positive.iloc[ind]['instant_satisfaction_score'] != all_sentiment_peak_positive_score):\n",
        "                all_sentiment_peak_positive_score = video_sentiment_positive.iloc[ind]['instant_satisfaction_score']\n",
        "                all_sentiment_positive_duration = 1\n",
        "            elif (video_sentiment_positive.iloc[ind]['instant_satisfaction_score'] == all_sentiment_peak_positive_score):\n",
        "                if (((ind-1) >= 0) and ((video_sentiment_positive.iloc[ind]['instant_offset'] == video_sentiment_positive.iloc[ind-1]['instant_offset']) or (video_sentiment_positive.iloc[ind]['instant_offset'] == ((video_sentiment_positive.iloc[ind-1]['instant_offset'])+1)))):\n",
        "                    all_sentiment_positive_duration += 1\n",
        "\n",
        "            if (((ind+1) == len(video_sentiment_positive)) or (((ind+1) < len(video_sentiment_positive)) and (video_sentiment_positive.iloc[ind+1]['instant_satisfaction_score'] != all_sentiment_peak_positive_score)) or (((ind+1) < len(video_sentiment_positive)) and ((((video_sentiment_positive.iloc[ind]['instant_offset'])+1) < video_sentiment_positive.iloc[ind+1]['instant_offset']))) ):\n",
        "                if (all_sentiment_positive_duration > 1):\n",
        "                    ind_time = ind-all_sentiment_positive_duration+1\n",
        "                else:\n",
        "                    ind_time = ind\n",
        "                all_sentiment_positive_duration_intervals.loc[len(all_sentiment_positive_duration_intervals.index)] = [video_sentiment_positive.iloc[ind]['valence'], video_sentiment_positive.iloc[ind_time]['time'], video_sentiment_positive.iloc[ind]['instant_satisfaction'], video_sentiment_positive.iloc[ind]['instant_satisfaction_score'], all_sentiment_positive_duration]\n",
        "                all_sentiment_positive_duration = 1\n",
        "\n",
        "        video_sentiment_negative.reset_index(drop=True, inplace=True)\n",
        "        for ind in video_sentiment_negative.index:\n",
        "            if (video_sentiment_negative.iloc[ind]['instant_satisfaction_score'] != all_sentiment_peak_negative_score):\n",
        "                all_sentiment_peak_negative_score = video_sentiment_negative.iloc[ind]['instant_satisfaction_score']\n",
        "                all_sentiment_negative_duration = 1\n",
        "            elif (video_sentiment_negative.iloc[ind]['instant_satisfaction_score'] == all_sentiment_peak_negative_score):\n",
        "                if (((ind-1) >= 0) and ((video_sentiment_negative.iloc[ind]['instant_offset'] == video_sentiment_negative.iloc[ind-1]['instant_offset']) or (video_sentiment_negative.iloc[ind]['instant_offset'] == ((video_sentiment_negative.iloc[ind-1]['instant_offset'])+1)))):\n",
        "                    all_sentiment_negative_duration += 1\n",
        "\n",
        "            if (((ind+1) == len(video_sentiment_negative)) or (((ind+1) < len(video_sentiment_negative)) and (video_sentiment_negative.iloc[ind+1]['instant_satisfaction_score'] != all_sentiment_peak_negative_score)) or (((ind+1) < len(video_sentiment_negative)) and ((((video_sentiment_negative.iloc[ind]['instant_offset'])+1) < video_sentiment_negative.iloc[ind+1]['instant_offset']))) ):\n",
        "                if (all_sentiment_negative_duration > 1):\n",
        "                    ind_time = ind-all_sentiment_negative_duration+1\n",
        "                else:\n",
        "                    ind_time = ind\n",
        "                all_sentiment_negative_duration_intervals.loc[len(all_sentiment_negative_duration_intervals.index)] = [video_sentiment_negative.iloc[ind]['valence'], video_sentiment_negative.iloc[ind_time]['time'], video_sentiment_negative.iloc[ind]['instant_satisfaction'], video_sentiment_negative.iloc[ind]['instant_satisfaction_score'], all_sentiment_negative_duration]\n",
        "                all_sentiment_negative_duration = 1\n",
        "\n",
        "\n",
        "        # Peaks video satisfaction\n",
        "        positive_sentiment_peak_identified = ''\n",
        "        max_duration_positive_sentiment_peak_identified = 0\n",
        "        max_instant_satisfaction_positive_sentiment_peak_identified = 0\n",
        "        for index, row in all_sentiment_positive_duration_intervals.iterrows():\n",
        "              if (max_instant_satisfaction_positive_sentiment_peak_identified < row['instant_satisfaction_score']):\n",
        "                  max_instant_satisfaction_positive_sentiment_peak_identified = row['instant_satisfaction_score']\n",
        "                  positive_sentiment_peak_identified = ''\n",
        "                  max_duration_positive_sentiment_peak_identified = row['duration']\n",
        "                  positive_sentiment_peak_identified += row['instant_satisfaction'] + '(video) - ' + row['time']\n",
        "                  if (index+1 < len(all_sentiment_positive_duration_intervals)):\n",
        "                      positive_sentiment_peak_identified += '\\n'\n",
        "\n",
        "              elif (max_instant_satisfaction_positive_sentiment_peak_identified == row['instant_satisfaction_score']):\n",
        "                  if (max_duration_positive_sentiment_peak_identified < row['duration']):\n",
        "                      positive_sentiment_peak_identified = ''\n",
        "                      max_duration_positive_sentiment_peak_identified = row['duration']\n",
        "                      positive_sentiment_peak_identified += row['instant_satisfaction'] + '(video) - ' + row['time']\n",
        "                      if (index+1 < len(all_sentiment_positive_duration_intervals)):\n",
        "                          positive_sentiment_peak_identified += '\\n'\n",
        "                  if (max_duration_positive_sentiment_peak_identified == row['duration']):\n",
        "                      positive_sentiment_peak_identified += row['instant_satisfaction'] + '(video) - ' + row['time']\n",
        "                      if (index+1 < len(all_sentiment_positive_duration_intervals)):\n",
        "                          positive_sentiment_peak_identified += '\\n'\n",
        "\n",
        "        negative_sentiment_peak_identified = ''\n",
        "        max_duration_negative_sentiment_peak_identified = 0\n",
        "        max_instant_satisfaction_negative_sentiment_peak_identified = 0\n",
        "        for index, row in all_sentiment_negative_duration_intervals.iterrows():\n",
        "              if (max_instant_satisfaction_negative_sentiment_peak_identified > row['instant_satisfaction_score']):\n",
        "                  max_instant_satisfaction_negative_sentiment_peak_identified = row['instant_satisfaction_score']\n",
        "                  negative_sentiment_peak_identified = ''\n",
        "                  max_duration_negative_sentiment_peak_identified = row['duration']\n",
        "                  negative_sentiment_peak_identified += row['instant_satisfaction'] + '(video) - ' + row['time']\n",
        "                  if (index+1 < len(all_sentiment_negative_duration_intervals)):\n",
        "                      negative_sentiment_peak_identified += '\\n'\n",
        "\n",
        "              elif (max_instant_satisfaction_negative_sentiment_peak_identified == row['instant_satisfaction_score']):\n",
        "                  if (max_duration_negative_sentiment_peak_identified > row['duration']):\n",
        "                      negative_sentiment_peak_identified = ''\n",
        "                      max_duration_negative_sentiment_peak_identified = row['duration']\n",
        "                      negative_sentiment_peak_identified += row['instant_satisfaction'] + '(video) - ' + row['time']\n",
        "                      if (index+1 < len(all_sentiment_negative_duration_intervals)):\n",
        "                          negative_sentiment_peak_identified += '\\n'\n",
        "                  if (max_duration_negative_sentiment_peak_identified == row['duration']):\n",
        "                      negative_sentiment_peak_identified += row['instant_satisfaction'] + '(video) - ' + row['time']\n",
        "                      if (index+1 < len(all_sentiment_negative_duration_intervals)):\n",
        "                          negative_sentiment_peak_identified += '\\n'\n",
        "\n",
        "        # All video satisfaction\n",
        "        all_positive_sentiment_peak_identified = ''\n",
        "        for index, row in all_sentiment_positive_duration_intervals.iterrows():\n",
        "              all_positive_sentiment_peak_identified += row['instant_satisfaction'] + '(video) - ' + row['time']\n",
        "              if (index+1 < len(all_sentiment_positive_duration_intervals)):\n",
        "                  all_positive_sentiment_peak_identified += '\\n'\n",
        "\n",
        "        all_negative_sentiment_peak_identified = ''\n",
        "        for index, row in all_sentiment_negative_duration_intervals.iterrows():\n",
        "              all_negative_sentiment_peak_identified += row['instant_satisfaction'] + '(video) - ' + row['time']\n",
        "              if (index+1 < len(all_sentiment_negative_duration_intervals)):\n",
        "                  all_negative_sentiment_peak_identified += '\\n'\n",
        "\n",
        "\n",
        "        print(\"Find positive and negative affect... \" + experiment_id)\n",
        "\n",
        "        sentiment_sentences_offset = subtitles_sentence_offset.copy()\n",
        "        sentiment_sentences_offset.drop(columns=['sentence', 'end_offset', 'fps'], inplace=True)\n",
        "\n",
        "        # Affect Peaks\n",
        "        # Identifying list of positive and negative offsets peaks\n",
        "        positive_sentiment_sentences_offset = sentiment_sentences_offset.drop(sentiment_sentences_offset[sentiment_sentences_offset['sentence_sentiment'] == 'Neutral'].index)\n",
        "        positive_sentiment_sentences_offset = positive_sentiment_sentences_offset.drop(positive_sentiment_sentences_offset[positive_sentiment_sentences_offset['sentence_sentiment'] == 'Negative'].index)\n",
        "\n",
        "        positive_peak_sentiment_sentences_offset = pd.DataFrame(data=None, columns=positive_sentiment_sentences_offset.columns)\n",
        "        max_positive_sentiment_magnitude = 0\n",
        "        for idx, row in positive_sentiment_sentences_offset.iterrows():\n",
        "            if (max_positive_sentiment_magnitude < row['sentiment_magnitude']):\n",
        "                positive_peak_sentiment_sentences_offset = pd.DataFrame(data=None, columns=positive_sentiment_sentences_offset.columns)\n",
        "                max_positive_sentiment_magnitude = row['sentiment_magnitude']\n",
        "                positive_peak_sentiment_sentences_offset.loc[len(positive_peak_sentiment_sentences_offset.index)] = [row['start_offset'], row['sentence_sentiment'], row['sentiment_magnitude']]\n",
        "            elif (max_positive_sentiment_magnitude == row['sentiment_magnitude']):\n",
        "                positive_peak_sentiment_sentences_offset.loc[len(positive_peak_sentiment_sentences_offset.index)] = [row['start_offset'], row['sentence_sentiment'], row['sentiment_magnitude']]\n",
        "\n",
        "        negative_sentiment_sentences_offset = sentiment_sentences_offset.drop(sentiment_sentences_offset[sentiment_sentences_offset['sentence_sentiment'] == 'Neutral'].index)\n",
        "        negative_sentiment_sentences_offset = negative_sentiment_sentences_offset.drop(negative_sentiment_sentences_offset[negative_sentiment_sentences_offset['sentence_sentiment'] == 'Positive'].index)\n",
        "\n",
        "        negative_peak_sentiment_sentences_offset = pd.DataFrame(data=None, columns=negative_sentiment_sentences_offset.columns)\n",
        "        max_negative_sentiment_magnitude = 0\n",
        "        for idx, row in negative_sentiment_sentences_offset.iterrows():\n",
        "            if (max_negative_sentiment_magnitude < row['sentiment_magnitude']):\n",
        "                negative_peak_sentiment_sentences_offset = pd.DataFrame(data=None, columns=negative_sentiment_sentences_offset.columns)\n",
        "                max_negative_sentiment_magnitude = row['sentiment_magnitude']\n",
        "                negative_peak_sentiment_sentences_offset.loc[len(negative_peak_sentiment_sentences_offset.index)] = [row['start_offset'], row['sentence_sentiment'], row['sentiment_magnitude']]\n",
        "            elif (max_negative_sentiment_magnitude == row['sentiment_magnitude']):\n",
        "                negative_peak_sentiment_sentences_offset.loc[len(negative_peak_sentiment_sentences_offset.index)] = [row['start_offset'], row['sentence_sentiment'], row['sentiment_magnitude']]\n",
        "\n",
        "\n",
        "        # Create a string with the speech sentiment peaks\n",
        "        idx_peak_offset_positive = 0\n",
        "        if ((positive_sentiment_peak_identified != '') and (positive_sentiment_peak_identified[len(positive_sentiment_peak_identified)-1] != '\\n')):\n",
        "            positive_sentiment_peak_identified += '\\n'\n",
        "        for idx, row in positive_peak_sentiment_sentences_offset.iterrows():\n",
        "            frames_peak = row['start_offset']\n",
        "            minutes_peak = int(frames_peak / 60)\n",
        "            seconds_peak = int(frames_peak % 60)\n",
        "            time_peak = str(minutes_peak) + \"m\" + str(seconds_peak) + \"s\"\n",
        "            positive_peak_offset = str(row['sentence_sentiment']) + \" (speech) - \" +  time_peak\n",
        "\n",
        "            positive_sentiment_peak_identified += positive_peak_offset\n",
        "            if ((idx_peak_offset_positive+1 < len(positive_peak_sentiment_sentences_offset))):\n",
        "                positive_sentiment_peak_identified += '\\n'\n",
        "            if (positive_sentiment_peak_identified == ''):\n",
        "                positive_sentiment_peak_identified = 'None'\n",
        "            idx_peak_offset_positive += 1\n",
        "\n",
        "        idx_peak_offset_negative = 0\n",
        "        if ((negative_sentiment_peak_identified != '') and (negative_sentiment_peak_identified[len(negative_sentiment_peak_identified)-1] != '\\n')):\n",
        "            negative_sentiment_peak_identified += '\\n'\n",
        "        for idx, row in negative_peak_sentiment_sentences_offset.iterrows():\n",
        "            frames_peak = row['start_offset']\n",
        "            minutes_peak = int(frames_peak / 60)\n",
        "            seconds_peak = int(frames_peak % 60)\n",
        "            time_peak = str(minutes_peak) + \"m\" + str(seconds_peak) + \"s\"\n",
        "            negative_peak_offset = str(row['sentence_sentiment']) + \" (speech) - \" +  time_peak\n",
        "\n",
        "            negative_sentiment_peak_identified += negative_peak_offset\n",
        "            if ((idx_peak_offset_negative+1 < len(negative_peak_sentiment_sentences_offset))):\n",
        "                negative_sentiment_peak_identified += '\\n'\n",
        "            if (negative_sentiment_peak_identified == ''):\n",
        "                negative_sentiment_peak_identified = 'None'\n",
        "            idx_peak_offset_negative += 1\n",
        "\n",
        "        # All speech sentiment\n",
        "        idx_all_affect_positive = 0\n",
        "        if (all_positive_sentiment_peak_identified == ''):\n",
        "            all_positive_sentiment_peak_identified_tmp = ''\n",
        "        else:\n",
        "            all_positive_sentiment_peak_identified_tmp = all_positive_sentiment_peak_identified\n",
        "            all_positive_sentiment_peak_identified_tmp += '\\n'\n",
        "        for idx_all_affect, row in positive_sentiment_sentences_offset.iterrows():\n",
        "            frames_peak = row['start_offset']\n",
        "            minutes_peak = int(frames_peak / 60)\n",
        "            seconds_peak = int(frames_peak % 60)\n",
        "            time_peak = str(minutes_peak) + \"m\" + str(seconds_peak) + \"s\"\n",
        "\n",
        "            positive_peak_offset = str(row['sentence_sentiment']) + \" (speech) - \" +  time_peak\n",
        "            all_positive_sentiment_peak_identified_tmp += positive_peak_offset\n",
        "\n",
        "            if ((idx_all_affect_positive+1 < len(positive_sentiment_sentences_offset))):\n",
        "                all_positive_sentiment_peak_identified_tmp += '\\n'\n",
        "            idx_all_affect_positive += 1\n",
        "        if (all_positive_sentiment_peak_identified_tmp == ''):\n",
        "            all_positive_sentiment_peak_identified_tmp = 'None'\n",
        "        all_positive_sentiment_peak_identified = all_positive_sentiment_peak_identified_tmp\n",
        "\n",
        "        idx_all_affect_negative = 0\n",
        "        if (all_negative_sentiment_peak_identified == ''):\n",
        "            all_negative_sentiment_peak_identified_tmp = ''\n",
        "        else:\n",
        "            all_negative_sentiment_peak_identified_tmp = all_negative_sentiment_peak_identified\n",
        "            all_negative_sentiment_peak_identified_tmp += '\\n'\n",
        "        for idx_all_affect, row in negative_sentiment_sentences_offset.iterrows():\n",
        "            frames_peak = row['start_offset']\n",
        "            minutes_peak = int(frames_peak / 60)\n",
        "            seconds_peak = int(frames_peak % 60)\n",
        "            time_peak = str(minutes_peak) + \"m\" + str(seconds_peak) + \"s\"\n",
        "\n",
        "            negative_peak_offset = str(row['sentence_sentiment']) + \" (speech) - \" +  time_peak\n",
        "            all_negative_sentiment_peak_identified_tmp += negative_peak_offset\n",
        "\n",
        "            if ((idx_all_affect_negative+1 < len(negative_sentiment_sentences_offset))):\n",
        "                all_negative_sentiment_peak_identified_tmp += '\\n'\n",
        "            idx_all_affect_negative += 1\n",
        "        if (all_negative_sentiment_peak_identified_tmp == ''):\n",
        "            all_negative_sentiment_peak_identified_tmp = 'None'\n",
        "        all_negative_sentiment_peak_identified = all_negative_sentiment_peak_identified_tmp\n",
        "\n",
        "        print(\"Evaluating affect ... \" + experiment_id)\n",
        "\n",
        "        sentiment_positive_quantity = len(all_sentiment_positive_duration_intervals) + len(positive_sentiment_sentences_offset)\n",
        "        sentiment_negative_quantity = len(all_sentiment_negative_duration_intervals) + len(negative_sentiment_sentences_offset)\n",
        "        total_sentiment_quantity = sentiment_positive_quantity + sentiment_negative_quantity\n",
        "        if (total_sentiment_quantity == 0):\n",
        "            total_sentiment_quantity = 1\n",
        "        positive_affect_identified = sentiment_positive_quantity / total_sentiment_quantity\n",
        "        negative_affect_identified = sentiment_negative_quantity / total_sentiment_quantity\n",
        "        affect_identified = ''\n",
        "        if (positive_affect_identified >= 0.65):\n",
        "            affect_identified = 'Positive'\n",
        "        elif (negative_affect_identified >= 0.65):\n",
        "            affect_identified = 'Negative'\n",
        "        else:\n",
        "            affect_identified = 'Neutral'\n",
        "\n",
        "        print(\"Creating evaluated video and audio... \" + experiment_id)\n",
        "\n",
        "        audio_evaluated = mp.AudioFileClip(audio_input)\n",
        "        video_evaluated = mp.VideoFileClip(video_output)\n",
        "        final = video_evaluated.set_audio(audio_evaluated)\n",
        "        evaluated_video_name = experiment_id + \" - Evaluated.mp4\"\n",
        "        final.write_videofile(evaluated_video_name, threads = 16, fps=fps, preset='ultrafast', verbose=False, logger=None)\n",
        "\n",
        "        print(\"Evaluating usability... \" + experiment_id)\n",
        "\n",
        "        sus_q1 = original_data_experiment['I think that I would like to use this system frequently'][index_experiment]\n",
        "        match sus_q1:\n",
        "            case 'Strongly agree':\n",
        "                sus_q1_score = 5\n",
        "            case 'Agree':\n",
        "                sus_q1_score = 4\n",
        "            case 'Neither agree nor disagree':\n",
        "                sus_q1_score = 3\n",
        "            case 'Disagree':\n",
        "                sus_q1_score = 2\n",
        "            case 'Strongly disagree':\n",
        "                sus_q1_score = 1\n",
        "\n",
        "        sus_q2 = original_data_experiment['I found the system unnecessarily complex'][index_experiment]\n",
        "        match sus_q2:\n",
        "            case 'Strongly agree':\n",
        "                sus_q2_score = 5\n",
        "            case 'Agree':\n",
        "                sus_q2_score = 4\n",
        "            case 'Neither agree nor disagree':\n",
        "                sus_q2_score = 3\n",
        "            case 'Disagree':\n",
        "                sus_q2_score = 2\n",
        "            case 'Strongly disagree':\n",
        "                sus_q2_score = 1\n",
        "\n",
        "        sus_q3 = original_data_experiment['I thought the system was easy to use'][index_experiment]\n",
        "        match sus_q3:\n",
        "            case 'Strongly agree':\n",
        "                sus_q3_score = 5\n",
        "            case 'Agree':\n",
        "                sus_q3_score = 4\n",
        "            case 'Neither agree nor disagree':\n",
        "                sus_q3_score = 3\n",
        "            case 'Disagree':\n",
        "                sus_q3_score = 2\n",
        "            case 'Strongly disagree':\n",
        "                sus_q3_score = 1\n",
        "\n",
        "        sus_q4 = original_data_experiment['I think that I would need the support of a technical person to be able to use this system'][index_experiment]\n",
        "        match sus_q4:\n",
        "            case 'Strongly agree':\n",
        "                sus_q4_score = 5\n",
        "            case 'Agree':\n",
        "                sus_q4_score = 4\n",
        "            case 'Neither agree nor disagree':\n",
        "                sus_q4_score = 3\n",
        "            case 'Disagree':\n",
        "                sus_q4_score = 2\n",
        "            case 'Strongly disagree':\n",
        "                sus_q4_score = 1\n",
        "\n",
        "        sus_q5 = original_data_experiment['I found the various functions in this system were well-integrated'][index_experiment]\n",
        "        match sus_q5:\n",
        "            case 'Strongly agree':\n",
        "                sus_q5_score = 5\n",
        "            case 'Agree':\n",
        "                sus_q5_score = 4\n",
        "            case 'Neither agree nor disagree':\n",
        "                sus_q5_score = 3\n",
        "            case 'Disagree':\n",
        "                sus_q5_score = 2\n",
        "            case 'Strongly disagree':\n",
        "                sus_q5_score = 1\n",
        "\n",
        "        sus_q6 = original_data_experiment['I thought there was too much inconsistency in this system'][index_experiment]\n",
        "        match sus_q6:\n",
        "            case 'Strongly agree':\n",
        "                sus_q6_score = 5\n",
        "            case 'Agree':\n",
        "                sus_q6_score = 4\n",
        "            case 'Neither agree nor disagree':\n",
        "                sus_q6_score = 3\n",
        "            case 'Disagree':\n",
        "                sus_q6_score = 2\n",
        "            case 'Strongly disagree':\n",
        "                sus_q6_score = 1\n",
        "\n",
        "        sus_q7 = original_data_experiment['I would imagine that most people would learn to use this system very quickly'][index_experiment]\n",
        "        match sus_q7:\n",
        "            case 'Strongly agree':\n",
        "                sus_q7_score = 5\n",
        "            case 'Agree':\n",
        "                sus_q7_score = 4\n",
        "            case 'Neither agree nor disagree':\n",
        "                sus_q7_score = 3\n",
        "            case 'Disagree':\n",
        "                sus_q7_score = 2\n",
        "            case 'Strongly disagree':\n",
        "                sus_q7_score = 1\n",
        "\n",
        "        sus_q8 = original_data_experiment['I found the system very cumbersome to use'][index_experiment]\n",
        "        match sus_q8:\n",
        "            case 'Strongly agree':\n",
        "                sus_q8_score = 5\n",
        "            case 'Agree':\n",
        "                sus_q8_score = 4\n",
        "            case 'Neither agree nor disagree':\n",
        "                sus_q8_score = 3\n",
        "            case 'Disagree':\n",
        "                sus_q8_score = 2\n",
        "            case 'Strongly disagree':\n",
        "                sus_q8_score = 1\n",
        "\n",
        "        sus_q9 = original_data_experiment['I felt very confident using the system'][index_experiment]\n",
        "        match sus_q9:\n",
        "            case 'Strongly agree':\n",
        "                sus_q9_score = 5\n",
        "            case 'Agree':\n",
        "                sus_q9_score = 4\n",
        "            case 'Neither agree nor disagree':\n",
        "                sus_q9_score = 3\n",
        "            case 'Disagree':\n",
        "                sus_q9_score = 2\n",
        "            case 'Strongly disagree':\n",
        "                sus_q9_score = 1\n",
        "\n",
        "        sus_q10 = original_data_experiment['I needed to learn a lot of things before I could get going with this system'][index_experiment]\n",
        "        match sus_q10:\n",
        "            case 'Strongly agree':\n",
        "                sus_q10_score = 5\n",
        "            case 'Agree':\n",
        "                sus_q10_score = 4\n",
        "            case 'Neither agree nor disagree':\n",
        "                sus_q10_score = 3\n",
        "            case 'Disagree':\n",
        "                sus_q10_score = 2\n",
        "            case 'Strongly disagree':\n",
        "                sus_q10_score = 1\n",
        "\n",
        "        sus_informed_score = 2.5 * ((sus_q1_score - 1) + (sus_q3_score - 1) + (sus_q5_score - 1) + (sus_q7_score - 1) + (sus_q9_score - 1) + (5 - sus_q2_score) + (5 - sus_q4_score) + (5 - sus_q6_score) + (5 - sus_q8_score) + (5 - sus_q10_score))\n",
        "        usability_informed = ''\n",
        "\n",
        "        if ((sus_informed_score >= 65)):\n",
        "                usability_informed = 'Good'\n",
        "        elif ((sus_informed_score >= 35) and (sus_informed_score < 65)):\n",
        "                usability_informed = 'Neutral'\n",
        "        else:\n",
        "                usability_informed = 'Poor'\n",
        "\n",
        "        sus_identified_score = 2.5 * ((sus_identified['score'][0] - 1) + (sus_identified['score'][2] - 1) + (sus_identified['score'][4] - 1) + (sus_identified['score'][6] - 1) + (sus_identified['score'][8] - 1) + (5 - sus_identified['score'][1]) + (5 - sus_identified['score'][3]) + (5 - sus_identified['score'][5]) + (5 - sus_identified['score'][7]) + (5 - sus_identified['score'][9]))\n",
        "        usability_identified = ''\n",
        "\n",
        "        if (sus_identified_score >= 65):\n",
        "                usability_identified = 'Good'\n",
        "        elif ((sus_identified_score >= 35) and (sus_identified_score < 65)):\n",
        "                usability_identified = 'Neutral'\n",
        "        else:\n",
        "                usability_identified = 'Poor'\n",
        "\n",
        "        print(\"Evaluating User Experience... \" + experiment_id)\n",
        "        user_experience_evaluation_score = 0\n",
        "        match usability_identified:\n",
        "            case 'Good':\n",
        "                user_experience_evaluation_score += 1\n",
        "            case 'Poor':\n",
        "                user_experience_evaluation_score -= 1\n",
        "\n",
        "        match affect_identified:\n",
        "            case 'Positive':\n",
        "                user_experience_evaluation_score += 1\n",
        "            case 'Negative':\n",
        "                user_experience_evaluation_score -= 1\n",
        "\n",
        "        match user_value_identified:\n",
        "            case 'Satisfied':\n",
        "                user_experience_evaluation_score += 1\n",
        "            case 'Dissatisfied':\n",
        "                user_experience_evaluation_score -= 1\n",
        "\n",
        "        user_experience_evaluation = ''\n",
        "        if ((user_experience_evaluation_score >= 2)):\n",
        "            user_experience_evaluation = 'Good'\n",
        "        elif ((user_experience_evaluation_score >= -1) and (user_experience_evaluation_score < 2)):\n",
        "            user_experience_evaluation = 'Neither good nor poor'\n",
        "        else:\n",
        "            user_experience_evaluation = 'Poor'\n",
        "\n",
        "        print(\"Preparing to upload evaluated subtitle, audio and video... \" + experiment_id)\n",
        "\n",
        "        creds, _ = default()\n",
        "        prefix = 'https://drive.google.com/file/d/'\n",
        "        suffix = '/view?usp=drive_link'\n",
        "        evaluated_video_filetype = 'video/mp4'\n",
        "        evaluated_video_folder_id = \"\" #change\n",
        "        if (evaluated_video_url != ''):\n",
        "            evaluated_video_id = evaluated_video_url.split('/')[-2]\n",
        "            evaluated_video_id = upload_basic(evaluated_video_name, evaluated_video_filetype, evaluated_video_folder_id, evaluated_video_id)\n",
        "        else:\n",
        "            evaluated_video_id = ''\n",
        "            evaluated_video_id = upload_basic(evaluated_video_name, evaluated_video_filetype, evaluated_video_folder_id, evaluated_video_id)\n",
        "            evaluated_video_url = prefix + evaluated_video_id + suffix\n",
        "\n",
        "        evaluated_audio_filetype = 'audio/wav'\n",
        "        evaluated_audio_folder_id = \"\" #change\n",
        "        if (evaluated_audio_url != ''):\n",
        "            evaluated_audio_id = evaluated_audio_url.split('/')[-2]\n",
        "            evaluated_audio_id = upload_basic(audio_input, evaluated_audio_filetype, evaluated_audio_folder_id, evaluated_audio_id)\n",
        "        else:\n",
        "            evaluated_audio_id = ''\n",
        "            evaluated_audio_id = upload_basic(audio_input, evaluated_audio_filetype, evaluated_audio_folder_id, evaluated_audio_id)\n",
        "            evaluated_audio_url = prefix + evaluated_audio_id + suffix\n",
        "\n",
        "        evaluated_subtitle_filetype = 'text/srt'\n",
        "        evaluated_subtitle_folder_id = \"\" #change\n",
        "        if (evaluated_subtitle_url != ''):\n",
        "            evaluated_subtitle_id = evaluated_subtitle_url.split('/')[-2]\n",
        "            evaluated_subtitle_id = upload_basic(subtitle_output, evaluated_subtitle_filetype, evaluated_subtitle_folder_id, evaluated_subtitle_id)\n",
        "        else:\n",
        "            evaluated_subtitle_id = ''\n",
        "            evaluated_subtitle_id = upload_basic(subtitle_output, evaluated_subtitle_filetype, evaluated_subtitle_folder_id, evaluated_subtitle_id)\n",
        "            evaluated_subtitle_url = prefix + evaluated_subtitle_id + suffix\n",
        "\n",
        "        evaluated_ai_sus_sentences_filetype = 'text/srt'\n",
        "        evaluated_ai_sus_sentences_folder_id = \"\" #change\n",
        "        if (evaluated_ai_sus_sentences_url != ''):\n",
        "            evaluated_ai_sus_sentences_id = evaluated_ai_sus_sentences_url.split('/')[-2]\n",
        "            evaluated_ai_sus_sentences_id = upload_basic(ai_sus_sentences_filename, evaluated_ai_sus_sentences_filetype, evaluated_ai_sus_sentences_folder_id, evaluated_ai_sus_sentences_id)\n",
        "        else:\n",
        "            evaluated_ai_sus_sentences_id = ''\n",
        "            evaluated_ai_sus_sentences_id = upload_basic(ai_sus_sentences_filename, evaluated_ai_sus_sentences_filetype, evaluated_ai_sus_sentences_folder_id, evaluated_ai_sus_sentences_id)\n",
        "            evaluated_ai_sus_sentences_url = prefix + evaluated_ai_sus_sentences_id + suffix\n",
        "\n",
        "\n",
        "        print(\"Sharing evaluated subtitle, audio, and video... \" + experiment_id)\n",
        "        creds, _ = default()\n",
        "        share_file(\n",
        "              real_file_id=evaluated_video_id,\n",
        "              real_user=participant_id,\n",
        "              real_domain=\"uxapp.com.br\",\n",
        "          )\n",
        "\n",
        "        share_file(\n",
        "              real_file_id=evaluated_audio_id,\n",
        "              real_user=participant_id,\n",
        "              real_domain=\"uxapp.com.br\",\n",
        "          )\n",
        "\n",
        "        share_file(\n",
        "            real_file_id=evaluated_subtitle_id,\n",
        "            real_user=participant_id,\n",
        "            real_domain=\"uxapp.com.br\",\n",
        "        )\n",
        "\n",
        "        print(\"Deleting audio from bucket... \" + experiment_id)\n",
        "\n",
        "        delete_blob(audio_bucket_name, audio_blob_name)\n",
        "        print(\"Preparing data to update... \" + experiment_id)\n",
        "        tz = pytz.timezone('America/Sao_Paulo')\n",
        "        now = datetime.now(tz=tz)\n",
        "        current_time = now.strftime(\"%H:%M:%S\")\n",
        "        job_end_time = current_time\n",
        "        print(\"Current Time =\", job_end_time)\n",
        "\n",
        "        original_data_experiment.loc[int(index_experiment), 'Evaluated Link'] = evaluated_video_url\n",
        "        original_data_experiment.loc[int(index_experiment), 'Evaluated Audio Link'] = evaluated_audio_url\n",
        "        original_data_experiment.loc[int(index_experiment), 'Evaluated Subtitle Link'] = evaluated_subtitle_url\n",
        "        original_data_experiment.loc[int(index_experiment), 'Evaluated AI SUS Sentences Link'] = evaluated_ai_sus_sentences_url\n",
        "        original_data_experiment.loc[int(index_experiment), 'Positive Sentiment Peak Identified'] = positive_sentiment_peak_identified\n",
        "        original_data_experiment.loc[int(index_experiment), 'Negative Sentiment Peak Identified'] = negative_sentiment_peak_identified\n",
        "        original_data_experiment.loc[int(index_experiment), 'All Positive Sentiment Identified'] = all_positive_sentiment_peak_identified\n",
        "        original_data_experiment.loc[int(index_experiment), 'All Negative Sentiment Identified'] = all_negative_sentiment_peak_identified\n",
        "        original_data_experiment.loc[int(index_experiment), 'User Value Identified'] = user_value_identified\n",
        "        original_data_experiment.loc[int(index_experiment), 'Usability Informed'] = usability_informed\n",
        "        original_data_experiment.loc[int(index_experiment), 'Usability Identified'] = usability_identified\n",
        "        original_data_experiment.loc[int(index_experiment), 'Usability Score Informed (SUS)'] = sus_informed_score\n",
        "        original_data_experiment.loc[int(index_experiment), 'Usability Score Identified (SUS)'] = sus_identified_score\n",
        "        original_data_experiment.loc[int(index_experiment), 'Affect Identified'] = affect_identified\n",
        "        original_data_experiment.loc[int(index_experiment), 'User Experience Evaluation'] = user_experience_evaluation\n",
        "        original_data_experiment.loc[int(index_experiment), 'Job Start'] = job_start_time\n",
        "        original_data_experiment.loc[int(index_experiment), 'Job End'] = job_end_time\n",
        "        original_data_experiment.loc[int(index_experiment), 'Status Script'] = 'DONE'\n",
        "        original_data_experiment.loc[int(index_experiment), 'SUS Identified Q1'] = int(sus_identified['score'][0])\n",
        "        original_data_experiment.loc[int(index_experiment), 'SUS Identified Q2'] = int(sus_identified['score'][1])\n",
        "        original_data_experiment.loc[int(index_experiment), 'SUS Identified Q3'] = int(sus_identified['score'][2])\n",
        "        original_data_experiment.loc[int(index_experiment), 'SUS Identified Q4'] = int(sus_identified['score'][3])\n",
        "        original_data_experiment.loc[int(index_experiment), 'SUS Identified Q5'] = int(sus_identified['score'][4])\n",
        "        original_data_experiment.loc[int(index_experiment), 'SUS Identified Q6'] = int(sus_identified['score'][5])\n",
        "        original_data_experiment.loc[int(index_experiment), 'SUS Identified Q7'] = int(sus_identified['score'][6])\n",
        "        original_data_experiment.loc[int(index_experiment), 'SUS Identified Q8'] = int(sus_identified['score'][7])\n",
        "        original_data_experiment.loc[int(index_experiment), 'SUS Identified Q9'] = int(sus_identified['score'][8])\n",
        "        original_data_experiment.loc[int(index_experiment), 'SUS Identified Q10'] = int(sus_identified['score'][9])\n",
        "\n",
        "        values_update = []\n",
        "        row_update = []\n",
        "\n",
        "        row_update = original_data_experiment.iloc[int(index_experiment)].values.flatten().tolist()\n",
        "        values_update.append(row_update)\n",
        "        creds, _ = default()\n",
        "        gsheet_update_values(SPREADSHEET_ID, RANGE_NAME, \"USER_ENTERED\", values_update,)\n",
        "\n",
        "        print(\"End.\")\n",
        "\n",
        "    else:\n",
        "        product = 'Nothing to do.'\n",
        "        tasktime = \"00/00/0000 00:00:00\"\n",
        "\n",
        "    call_response = product + ' - ' + tasktime\n",
        "    return 'Product - Task time: {}!'.format(call_response)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}